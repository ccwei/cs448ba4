\documentclass{sigchi}

% Remove or comment out these two lines for final version
% \toappearbox{\Large Submitted to CHI'13. \\Do not cite, do not circulate.}
\pagenumbering{arabic}% Arabic page numbers for submission.

% Use \toappear{...} to override the default ACM copyright statement (e.g. for preprints).

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage{url}      % llt: nicely formatted URLs
\usepackage{cite}      % llt: nicely formatted URLs

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}


% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages,
% to give it a fighting chance of not being over-written,
% since its job is to redefine many LaTeX commands.
\usepackage[pdftex]{hyperref}
\hypersetup{
pdftitle={SIGCHI Conference Proceedings Format},
pdfauthor={LaTeX},
pdfkeywords={SIGCHI, proceedings, archival format},
bookmarksnumbered,
pdfstartview={FitH},
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black,
breaklinks=true,
}

% create a shortcut to typeset table headings
\newcommand\tabhead[1]{\small\textbf{#1}}


% End of preamble. Here it comes the document.
\begin{document}

\title{Peereviz: Visualizing Peer Reviews}

% Note that submissions are blind, so author information should be omitted
\numberofauthors{3}
\author{
  \alignauthor Chih-Chiang Wei\\
    \affaddr{Computer Science Department}\\
    \affaddr{Stanford University}\\
    \email{ccwei@stanford.edu}\\
  \alignauthor Kanit Wongsuphasawat\\
    \affaddr{Management Science \& Engineering Department}\\
    \affaddr{Stanford University}\\
    \email{kanitw@stanford.edu}\\
  \alignauthor Thiraphat Charoensripongsa\\
    \affaddr{Computer Science Department}\\
    \affaddr{Stanford University}\\
    \email{tchar@stanford.edu}
}

% Teaser figure can go here
%\teaser{
%  \centering
%  \includegraphics{Figure1}
%  \caption{Teaser Image}
%  \label{fig:teaser}
%}

\maketitle

\begin{abstract}

% TODO rewrite this at the end!

Peer reviews are important and commonly used in many project-based classes. In
massive online open classrooms (MOOCs), the data from peer review process is
huge and it becomes very difficult to explore and understand. Our goal is to
design a visualization tool for large-scale peer review exploration which helps
course instructors to understand and gain insights from the peer review
activities, the engagement of students, and the quality of the reviews. We
utilize existing text visualization techniques as well as multi-view
coordination and enable the navigation through score distributions and the text
feedbacks.

\end{abstract}

\keywords{
	peer review; group collaboration; visualization; MOOC; exploration;
  text visualization
}
	% \\\textcolor{red}{Mandatory section to be included in your final version.}
% }

% \category{H.5.m.}{Information Interfaces and Presentation (e.g. HCI)}{Miscellaneous
% \\
% \textcolor{red}{See: \url{http://www.acm.org/about/class/1998/}
% for more information and the full list of ACM classifiers and descriptors.
% Mandatory section: On the submission page
% only the classifiers' letter-number combination will need to be entered.}
% }

% \terms{
% 	Human Factors; Design; Measurement.
% 	If you choose more than one ACM General Term,
% 	separate the terms with a semi-colon.
% \\
% \textcolor{red}{If you choose more than one ACM General Term,
% separate the terms with a semi-colon. See list of ACM terms at:
% \url{http://www.sheridanprinting.com/sigchi/generalterms.htm}.
% Optional section to be included in your final version.}
% }

\section{Introduction}
Massive open online courses(MOOCs) including Khan Academy, Udacity, Coursera and
Venture Lab are increasing in numbers. \cite{nytimes} One emerging challenge is
how to scale assessment to these high student-teacher ratio environments.

Using peer review system \cite{cpr} to evaluate students’ work is one possible solution.
However, in the massive online courses,
the number of reviews done in each class is large and it is overly difficult
and time-consuming for instructors to get a comprehensive understanding
of the review of each task.

To address this problem, we developed Peereviz, a peer review exploration tool
on top of Venture Lab platform. It aims for helping instructors to understand
the massive amount of peer review results. The design goal of the tool is to
create a visualization tool that helps course instructors in three ways:
1) get the overall understanding of peer review activities;
2) be able to dive into specific part of the review results;
3) quick browse and select higher quality reviews to read.

As a data explorer, it follows "overview first, zoom and filter, then details-
on-demand" mantra. \cite{card1999readings}


The paper organization is as follows.
We first summarize previous work in peer review system and visualization.
We then describe the peer review data we used.
Next, we present our design and visualization techniques,
followed by the evaluation and feedbacks from users.
Finally, we suggest the possible directions for future work
and then summarize our work.



\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{images/review-form}
\caption{the actual evaluation form on the Venture lab platform.
The top part shows the quantitative feedbacks which are score ranging
from Low (1) to High (10) while the bottom part shows the feedback capture grid
which a form where the reviewers give their opinions
on four different aspect: notable, constructive, questions, and ideas.}
\label{fig:review-form}
\end{figure}



\section{Related Work}

A number of studies has confirmed the usefulness of the peer review system
to the students in various domains including programming and writing class
\cite{MyPeerReview,WebBasedPeerReview}.

As for visualization, Multiple Coordinated Views is an exploratory visualization
technique that shows data in different representations, enabling users to
interact and explore intricate data to understand the data.  [State of the art:
coordinated \& multiple views....] And text visualization techniques including
word count list and tag cloud are often used to visualize and understand various
large text data.[Tag clouds for summarizing web search
results][http://www.wordcount.org/main.php]


However, to the best of our knowledge, work that combine visualization techniques
to help instructors understand peer review data is still limited.
The only work we discovered is  an interactive tool for peer-review exploration
proposed by Xiong et al. \cite{xiong}
They primarily work on the improvement of semantic information
to help instructor discover interesting patterns and
compare different groups of student in the writing class on SWoRD system \cite{Cho2007}.
In contrast to the prior work, we mainly focuses on the visualization
and interaction design for exploring multi-dimensional peer review data
in the recently emerging large-scale online classrooms.

describe: An Interactive Analytic Tool for Peer-Review Exploration How are we
different?: focus on multiview, navigation, qualitative + quantitative feedback


\begin{figure*}[!t]
\centering
\includegraphics[width=2.0\columnwidth]{images/overview-annotated}
\caption{The layout for Peereviz is divided into two parts:
the team browser (1,2) and the review browser (3,4).
(1) shows the overall score distribution,
(2) team description list,
(3) keyword list, and
(4) Aggregate and individual reviews.}
\label{fig:overview-annotated}
\end{figure*}

\section{Data Set Description}
We use a sample data set from a final project peer review result in the Spring
2012 Entrepreneurship class on the Venture Lab platform. A total of 1,206 peer
reviews are collected, which consists of 116 teams being reviewed and 212
individual reviewers. For privacy reasons, the team names are anonymized and we
presented team description as much as needed. The actual peer review form has
two parts, including (1) multiple quantitative score ranging from low(1) to
high(10) for different aspects of the final project and (2) qualitative text
feedback, which utilizes a feedback capture grid \cite{dbootcamp} where
reviewers could express their opinions in four dimensions–what they
like(\emph{notable}), constructive criticisms (\emph{constructive}), questions
they have about the work (\emph{questions}), and any additional ideas for the
project (\emph{ideas}).



\section{Visualization Design}

The layout as shown in Figure \ref{fig:overview-annotated} consists of two
parts, the team browser and the review browser.  The first is the team browser
on the left side, which contains a team list and a bar chart that visualizes
overall score distribution for all team projects. The second is the review
browser on the right side which consists of two sub-parts: the keyword
visualization for browsing keyword on the upper part and review display view on
the bottom part.

\begin{figure}[tb]
\centering
\includegraphics[width=\columnwidth]{images/3charts}
\caption{Different selection modes in the team browser.}
\label{fig:keyword-lists}
\end{figure}

\subsection{Team Browser}
The team browser serves as the main part of the overview
where we can select one or multiple teams using different ways of filtering.

We can select one team by clicking one of the small block on the stacked bar
chart or select multiple teams at the same time by brushing over the bar chart.
After the selection, the team descriptions will be listed below as noted by (2)
in Figure \ref{fig:overview-annotated}. The information provided in the review
browser will be updated accordingly with the data of the selected team(s). In
addition, it provides a search tool to select teams by their descriptions as
well.



\subsection{Review Browser}

The review browser shows data for the selected team(s) in the team browser. If
the user has not selected any team in the team browser, the review browser provides overview by showing aggregated data of all teams.

\subsubsection{Keyword List, Phrase List and Tag Clouds}

Since one major component of the review is qualitative text feedbacks,
we extracted keywords from text feedbacks based on their term frequency in bag-of-
words model \cite{bag-of-words} to provide an overview of what students
wrote the most in their reviews.
There are three representations: keyword list, phrase list and tag cloud.
In any of the three modes, users can click any word to see the actual occurrence of the word to further understand the context of the word.


\begin{figure*}[b]
\centering
\includegraphics[width=2.0\columnwidth]{images/phrase-list}
\includegraphics[width=2.0\columnwidth]{images/keyword-list}
\includegraphics[width=2.0\columnwidth]{images/clouds}
\caption{Three views of the keyword browser:
(top) the phrase list in integrated mode,
(middle) the keyword list in group-by-type mode,
(bottom) the tag clouds grouped by type.}
\label{fig:keyword-lists}
\end{figure*}

The keyword list and phrase list are basically frequency lists of unigram and
bigram respectively.  Each word in the frequently list show a histogram of their
frequencies on the top of the word.  These frequency lists have two views.  The
first view (Upper in Figure \ref{fig:keyword-lists}) show the total frequency of the
word aggregated from all the feedback types.  In this first view, we use color
to encoding the proportion of keyword found from each type on the histogram.
The second view shows keywords in four columns, in which each column represent
keywords from different types of feedback in the original feedback capture grid.
Both frequency list views show the most frequent words on the top.  User can
also use the search box to search for unigram or bigram they are interested in.
Meanwhile, the tag clouds view show four tag clouds of keywords group by each
dimension in the feedback capture grid.  Our tag clouds display all words
horizontally for easy reading.


\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{images/aggregate-view}
\caption{Aggregate View.}
\label{fig:aggregate-view}
\end{figure}

\subsubsection{Aggregate View and Individual Review View}

In aggregate view, all of the original reviews for the teams selected from team
browser are are shown categorized by their types from feedback capture grid.
This view provides instructors a quick way to read through a number of reviews
for each feedback type at the same time. As usual, we provide a search tool so
users can search original reviews.

To provide more details for each review, we can use individual review view
consisting of two main parts. First part on the left is a list of small
multiples each of which represents a review. We use color encoding in the small
grid to represent the length of the review text: the more intensity, the longer
the review text, so that the instructors can quickly see and select longer
reviews which are potentially more valuable to read. We can select small
multiple to navigate to the view on the right which is a list of detailed
review. Each item in the list contains all the the score distribution according
to the score rubrics, the average score, and the feedback capture grid as in the
original form.

From these extracted keywords, a keyword browser is provided to let users
quickly browse through the reviews.




\section{Implementation Notes}
Peereviz was written in HTML, CSS and Javascript using the d3.js visualization
toolkit \cite{d3} and a combination of javascript libraries including jQuery,
Underscore.js, Backbone.js [ref for each one].  The d3’s design that utilizes
existing web standards including SVG and HTML has greatly facilitated our
implementation.  Additionally, we performed data preprocessing and imported it
directly into an one-page web application.


% this can be 'Usage Observation' (http://www.danah.org/papers/InfoViz2005.pdf)
\section{Evaluation}

\section{Future Work}
... -> generalize for other platform

\section{Conclusion}
...

\section{Acknowledgments}
The authors of this paper would like to thank Farnaz Ronaghi and Venture Lab
team for providing data and suggestions.
Jeff Heer and CS448b (Data Visualization) course teaching team's comment
and suggestions are also appreciated.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
%\balance

% If you want to use smaller typesetting for the reference list,
% uncomment the following line:
% \small
\bibliographystyle{acm-sigchi}
\bibliography{peerevis}

\end{document}
